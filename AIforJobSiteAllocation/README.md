What if we can use AI to predict what is the best site where the glidein should be spawned?
Some sites give information about the number of resources that are going to provide for the job, while some others donâ€™t.
We want to allocate the glidein to the site that provide the largest amount of resources while minimizing the probability of failure.
- Predict the amount of CPU and Memory that is going to be provided from each site: ğ¶ğ‘ƒğ‘ˆğ‘ƒğ‘Ÿğ‘œğ‘£ğ‘–ğ‘‘ğ‘’ğ‘‘,ğ‘€ğ‘’ğ‘šğ‘œğ‘Ÿğ‘¦ğ‘ƒğ‘Ÿğ‘œğ‘£ğ‘–ğ‘‘ğ‘’ğ‘‘
- Consider the sites for which the resources provided are enough for our job: ğ¶ğ‘ƒğ‘ˆğ‘…ğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘’ğ‘‘ â‰¤ ğ¶ğ‘ƒğ‘ˆğ‘ƒğ‘Ÿğ‘œğ‘£ğ‘–ğ‘‘ğ‘’ğ‘‘ and ğ‘€ğ‘’ğ‘šğ‘œğ‘Ÿğ‘¦ğ‘…ğ‘’ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘’ğ‘‘ â‰¤ ğ‘€ğ‘’ğ‘šğ‘œğ‘Ÿğ‘¦ğ‘ƒğ‘Ÿğ‘œğ‘£ğ‘–ğ‘‘ğ‘’ğ‘‘.
- Calculate the probability of failure of each site: ğ‘ƒ_ğ‘“ğ‘ğ‘–ğ‘™ğ‘¢ğ‘Ÿğ‘’
- Calculate a cumulative score that allows us to take this decision

The memory and CPU amount forecasting is performed using a time series analysis (analysis in the time_series_analysis folder).
The probability calculation and failure prediction is performed with probablistic classifiers (analysis in the classification folder).
A scraper script is present in order to extract data.

Datasets can be found at: https://zenodo.org/record/7097223#.YynPwi9aZQI.
Download the datasets and place them in the data folder. 
The raw dataset should be autonomously scraped.
